<!DOCTYPE html>
<html lang=zh>
<head>
  
  <meta name="msvalidate.01" content="9F44BEDCB19BA7FC42CFA175FA23AEEF" />
  
  <meta name="google-site-verification" content="2dJwzsgMTepl0cllOC-3mrrpAZIZmiFdT3FxU6XHcOI" />
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>CMU-深度学习系统-第五章&amp;hw1 | Shuyu Zhang&#39;s Blog</title>
  <meta name="description" content="实现前向传播  PowerScalar EwiseDive DivScalar MatMul Summation BroadcastTo Reshape Negate Transpose Log Exp EWisePow  实现反向传播  PowerScalar EWiseDiv DivScalar MatMul Summation  假设前向">
<meta property="og:type" content="article">
<meta property="og:title" content="CMU-深度学习系统-第五章&amp;hw1">
<meta property="og:url" content="http://gladiouszhang.github.io/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="实现前向传播  PowerScalar EwiseDive DivScalar MatMul Summation BroadcastTo Reshape Negate Transpose Log Exp EWisePow  实现反向传播  PowerScalar EWiseDiv DivScalar MatMul Summation  假设前向">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2025-04-04T14:55:53.000Z">
<meta property="article:modified_time" content="2025-04-09T12:45:00.916Z">
<meta property="article:author" content="张舒俞">
<meta name="twitter:card" content="summary">
  <!-- Canonical links -->
  <link rel="canonical" href="http://gladiouszhang.github.io/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    <link href="//cdn.jsdelivr.net/npm/katex@0.9.0/dist/katex.min.css" rel="stylesheet">
  
  
  
  
<meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/gladiouszhang" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Shuyu Zhang (张舒俞)</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">学生</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Shaanxi, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-repository">
          <a href="/repository">
            
            <i class="icon icon-project"></i>
            
            <span class="menu-title">项目</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-books">
          <a href="/books">
            
            <i class="icon icon-book-fill"></i>
            
            <span class="menu-title">书单</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">友链</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">关于</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/gladiouszhang" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/n/Gladious_Zhang" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>欢迎交流与分享经验!</p>
            </div>
        </div>
    </div>
</div>

    
      

    
      
  <div class="widget">
    <h3 class="widget-title">标签</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/AI-Infra/" rel="tag">AI Infra</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Java/" rel="tag">Java</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/" rel="tag">分布式系统</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">学习笔记</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/" rel="tag">实习项目</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%91%84%E5%BD%B1/" rel="tag">摄影</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" rel="tag">边缘计算</a><span class="tag-list-count">3</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/AI-Infra/" style="font-size: 13.6px;">AI Infra</a> <a href="/tags/Java/" style="font-size: 13.6px;">Java</a> <a href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/" style="font-size: 13.8px;">分布式系统</a> <a href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 14px;">学习笔记</a> <a href="/tags/%E5%AE%9E%E4%B9%A0%E9%A1%B9%E7%9B%AE/" style="font-size: 13.2px;">实习项目</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13px;">强化学习</a> <a href="/tags/%E6%91%84%E5%BD%B1/" style="font-size: 13px;">摄影</a> <a href="/tags/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97/" style="font-size: 13.4px;">边缘计算</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">四月 2025</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">三月 2025</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/11/">十一月 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">八月 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">四月 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2025/04/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%85%AD%E7%AB%A0/" class="title">CMU-深度学习系统-第六章</a>
              </p>
              <p class="item-date">
                <time datetime="2025-04-09T03:30:01.000Z" itemprop="datePublished">2025-04-09</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/" class="title">CMU-深度学习系统-第五章&amp;hw1</a>
              </p>
              <p class="item-date">
                <time datetime="2025-04-04T14:55:53.000Z" itemprop="datePublished">2025-04-04</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2025/03/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-hw0/" class="title">CMU-深度学习系统-hw0</a>
              </p>
              <p class="item-date">
                <time datetime="2025-03-09T14:57:13.000Z" itemprop="datePublished">2025-03-09</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2025/03/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%9B%9B%E7%AB%A0/" class="title">CMU-深度学习系统-第四章</a>
              </p>
              <p class="item-date">
                <time datetime="2025-03-09T06:35:56.000Z" itemprop="datePublished">2025-03-09</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/2025/03/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%B8%89%E7%AB%A0/" class="title">CMU-深度学习系统-第三章</a>
              </p>
              <p class="item-date">
                <time datetime="2025-03-09T05:42:10.000Z" itemprop="datePublished">2025-03-09</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-CMU-深度学习系统-第五章-hw1" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      CMU-深度学习系统-第五章&amp;hw1
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/" class="article-date">
	  <time datetime="2025-04-04T14:55:53.000Z" itemprop="datePublished">2025-04-04</time>
	</a>
</span>
        
        

        
	<span class="article-read hidden-xs">
	    <i class="icon icon-eye-fill" aria-hidden="true"></i>
	    <span id="busuanzi_container_page_pv">
			<span id="busuanzi_value_page_pv">0</span>
		</span>
	</span>


        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/#comments" class="article-comment-link">评论</a></span>
        
	
		<span class="post-wordcount hidden-xs" itemprop="wordCount">字数统计: 4k(字)</span>
	
	
		<span class="post-readcount hidden-xs" itemprop="timeRequired">阅读时长: 18(分)</span>
	

      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <!-- toc -->
<ul>
<li><a href="#实现前向传播">实现前向传播</a>
<ul>
<li><a href="#powerscalar"><code>PowerScalar</code></a></li>
<li><a href="#ewisedive"><code>EwiseDive</code></a></li>
<li><a href="#divscalar"><code>DivScalar</code></a></li>
<li><a href="#matmul"><code>MatMul</code></a></li>
<li><a href="#summation"><code>Summation</code></a></li>
<li><a href="#broadcastto"><code>BroadcastTo</code></a></li>
<li><a href="#reshape"><code>Reshape</code></a></li>
<li><a href="#negate"><code>Negate</code></a></li>
<li><a href="#transpose"><code>Transpose</code></a></li>
<li><a href="#log"><code>Log</code></a></li>
<li><a href="#exp"><code>Exp</code></a></li>
<li><a href="#ewisepow"><code>EWisePow</code></a></li>
</ul></li>
<li><a href="#实现反向传播">实现反向传播</a>
<ul>
<li><a href="#powerscalar"><code>PowerScalar</code></a></li>
<li><a href="#ewisediv"><code>EWiseDiv</code></a></li>
<li><a href="#divscalar"><code>DivScalar</code></a></li>
<li><a href="#matmul"><code>MatMul</code></a></li>
<li><a href="#summation"><code>Summation</code></a>
<ul>
<li><a href="#假设前向传播的求和操作如下">假设前向传播的求和操作如下：</a></li>
<li><dl>
<dt><a href="#反向传播的关键分析">反向传播的关键分析</a></dt>
<dd>
<a href="#普通操作的情况对比">普通操作的情况（对比）</a>
</dd>
<dd>
<a href="#求和操作的特殊性">求和操作的特殊性</a>
</dd>
</dl></li>
</ul></li>
<li><a href="#broadcastto"><code>BroadcastTo</code></a></li>
<li><a href="#reshape"><code>Reshape</code></a></li>
<li><a href="#negate"><code>Negate</code></a></li>
<li><a href="#transpose"><code>Transpose</code></a></li>
<li><a href="#log"><code>Log</code></a></li>
<li><a href="#exp"><code>Exp</code></a></li>
<li><a href="#ewisepow"><code>EWisePow</code></a></li>
</ul></li>
<li><a href="#拓扑排序">拓扑排序</a></li>
<li><a href="#反向模式微分实现">反向模式微分实现</a></li>
<li><a href="#softmax-loss">Softmax Loss</a></li>
<li><a href="#用needle来重写sgd">用needle来重写SGD</a></li>
</ul>
<!-- tocstop -->
<p>由于第五章只是介绍了一下needle这个库，我觉得其实直接做hw1就行了，所以不再单独写第五章。</p>
<h1 id="实现前向传播">实现前向传播</h1>
<p>这个是最简单的，其实只需要一行左右的代码</p>
<h2 id="powerscalar"><code>PowerScalar</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a: NDArray</span>) -&gt; NDArray:</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> a**<span class="variable language_">self</span>.scalar</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="ewisedive"><code>EwiseDive</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> a/b</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="divscalar"><code>DivScalar</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> a/<span class="variable language_">self</span>.scalar</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="matmul"><code>MatMul</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a, b</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> a@b</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="summation"><code>Summation</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.<span class="built_in">sum</span>(a,axis=<span class="variable language_">self</span>.axes)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="broadcastto"><code>BroadcastTo</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.broadcast_to(a,<span class="variable language_">self</span>.shape)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="reshape"><code>Reshape</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.reshape(a,<span class="variable language_">self</span>.shape)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="negate"><code>Negate</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span> * a</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="transpose"><code>Transpose</code></h2>
<p>我感觉只有这个稍微复杂一点，原本寻思直接调用<code>array_api.transpose</code>，然而这个函数并不是这样用的，相比之下swapaxes函数更合理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.axes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> array_api.swapaxes(a, <span class="built_in">len</span>(a.shape)-<span class="number">1</span>,<span class="built_in">len</span>(a.shape)-<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> array_api.swapaxes(a, <span class="variable language_">self</span>.axes[<span class="number">0</span>],<span class="variable language_">self</span>.axes[<span class="number">1</span>])</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="log"><code>Log</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.log(a)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="exp"><code>Exp</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a</span>):</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.exp(a)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="ewisepow"><code>EWisePow</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute</span>(<span class="params">self, a: NDArray, b: NDArray</span>) -&gt; NDArray:</span><br><span class="line">        <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">        <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">        <span class="keyword">return</span> array_api.power(a,b)</span><br><span class="line">        <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h1 id="实现反向传播">实现反向传播</h1>
<p>backwards的函数形参有out_grad, node，其中out_grad代表的是这个计算结果的伴随。比如<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="9.531ex" height="2.009ex" role="img" focusable="false" viewbox="0 -694 4212.6 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mtext" transform="translate(529,0)"><path data-c="A0" d=""/></g><g data-mml-node="mi" transform="translate(779,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"/></g><g data-mml-node="mi" transform="translate(1264,0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g><g data-mml-node="mtext" transform="translate(1767,0)"><path data-c="A0" d=""/></g><g data-mml-node="mi" transform="translate(2017,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(2723.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(3779.6,0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g></g></g></svg></mjx-container></span>，且最终输出是<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewbox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>，那么out_grad就是<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.817ex;" xmlns="http://www.w3.org/2000/svg" width="2.685ex" height="3.058ex" role="img" focusable="false" viewbox="0 -990.5 1186.7 1351.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,485) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mrow" transform="translate(240.2,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"/></g></g></g><rect width="946.7" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span>；node代表的是这个运算节点本身。我们最终要输出的是这个运算的所有输入的部分伴随，根据<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.034ex;" xmlns="http://www.w3.org/2000/svg" width="12.333ex" height="3.418ex" role="img" focusable="false" viewbox="0 -1053.6 5451.1 1510.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="TeXAtom" transform="translate(518,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(345,0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1345,0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mo" transform="translate(0,375)"><svg width="1810.4" height="237" x="0" y="148" viewbox="452.6 148 1810.4 237"><path data-c="2013" d="M0 248V285H499V248H0Z" transform="scale(5.431,1)"/></svg></g></g><g data-mml-node="mo" transform="translate(2088.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mover" transform="translate(3143.9,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g><g data-mml-node="mo" transform="translate(0,375)"><svg width="859.3" height="237" x="0" y="148" viewbox="214.8 148 859.3 237"><path data-c="2013" d="M0 248V285H499V248H0Z" transform="scale(2.578,1)"/></svg></g></g><g data-mml-node="mfrac" transform="translate(4003.3,0)"><g data-mml-node="mrow" transform="translate(220,548.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"/></g></g></g><g data-mml-node="mrow" transform="translate(236.8,-345.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D715" d="M202 508Q179 508 169 520T158 547Q158 557 164 577T185 624T230 675T301 710L333 715H345Q378 715 384 714Q447 703 489 661T549 568T566 457Q566 362 519 240T402 53Q321 -22 223 -22Q123 -22 73 56Q42 102 42 148V159Q42 276 129 370T322 465Q383 465 414 434T455 367L458 378Q478 461 478 515Q478 603 437 639T344 676Q266 676 223 612Q264 606 264 572Q264 547 246 528T202 508ZM430 306Q430 372 401 400T333 428Q270 428 222 382Q197 354 183 323T150 221Q132 149 132 116Q132 21 232 21Q244 21 250 22Q327 35 374 112Q389 137 409 196T430 306Z"/></g><g data-mml-node="msub" transform="translate(566,0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><rect width="1207.9" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span>，就是用这个out_grad乘上针对这个运算的微分，在根据维度进行调整（反正我是这样做的）</p>
<p>这里面有几个函数的反向传播过程还挺迷惑的，但是我觉得重点应该看懂<code>test/hw1/test_autograd_hw.py</code>里面的<code>gradient_check</code>函数，这个函数会教会如何用数学推导来验证反向传播过程的正确性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_check</span>(<span class="params">f, *args, tol=<span class="number">1e-6</span>, backward=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">    eps = <span class="number">1e-4</span></span><br><span class="line">    numerical_grads = [np.zeros(a.shape) <span class="keyword">for</span> a <span class="keyword">in</span> args]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(args)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(args[i].realize_cached_data().size):</span><br><span class="line">            args[i].realize_cached_data().flat[j] += eps</span><br><span class="line">            f1 = <span class="built_in">float</span>(f(*args, **kwargs).numpy().<span class="built_in">sum</span>())</span><br><span class="line">            args[i].realize_cached_data().flat[j] -= <span class="number">2</span> * eps</span><br><span class="line">            f2 = <span class="built_in">float</span>(f(*args, **kwargs).numpy().<span class="built_in">sum</span>())</span><br><span class="line">            args[i].realize_cached_data().flat[j] += eps</span><br><span class="line">            numerical_grads[i].flat[j] = (f1 - f2) / (<span class="number">2</span> * eps)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> backward:</span><br><span class="line">        out = f(*args, **kwargs)</span><br><span class="line">        computed_grads = [</span><br><span class="line">            x.numpy()</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> out.op.gradient_as_tuple(ndl.Tensor(np.ones(out.shape)), out)</span><br><span class="line">        ]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        out = f(*args, **kwargs).<span class="built_in">sum</span>()</span><br><span class="line">        out.backward()</span><br><span class="line">        computed_grads = [a.grad.numpy() <span class="keyword">for</span> a <span class="keyword">in</span> args]</span><br><span class="line">    error = <span class="built_in">sum</span>(</span><br><span class="line">        np.linalg.norm(computed_grads[i] - numerical_grads[i]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(args))</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">assert</span> error &lt; tol</span><br><span class="line">    <span class="keyword">return</span> computed_grads</span><br></pre></td></tr></table></figure>
<p>以上函数的思路其实就是在各个输入上加上一个微小变量<code>eps</code>，然后求出变化后变量对应的函数值，<strong>再把这个函数值（矩阵）直接求和，得到一个常数</strong>；再使用这个输入减去微小变量<code>eps</code>，求出变化后变量对应的函数值，<strong>同样进行求和得到常数</strong>。将两个函数值相减，除以两倍的<code>eps</code>，这时候得到的就是针对这一个输入值（<strong>也就是矩阵里面单个元素</strong>）的数值的梯度，这里把多个元素凑一块，得到的是输入矩阵一样的大小的矩阵，这个矩阵也就是op的输出针对这个输入的梯度矩阵。在这里假设这个op计算的结果就是最终的输出结果，也即这个矩阵就是这个输入的伴随。</p>
<p>与此同时，因为这个函数是为了测试计算梯度是否与数值梯度相同，所以它将out_grad视为全1：<code>for x in out.op.gradient_as_tuple(ndl.Tensor(np.ones(out.shape)), out)</code>，因为如果op的计算结果就是最终解的话，那么这个out_grad就是全1，和上面保持一致，然后交给自己写的反向传播过程进行验证，得到的结果和数学解进行进行assert。</p>
<p>反向传播里面好几个比较抽象的，都是按这个方式来分析。函数里面具体说。</p>
<h2 id="powerscalar-1"><code>PowerScalar</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad * <span class="variable language_">self</span>.scalar * node.inputs[<span class="number">0</span>]**(<span class="variable language_">self</span>.scalar-<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="ewisediv"><code>EWiseDiv</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad/node.inputs[<span class="number">1</span>], out_grad*node.inputs[<span class="number">0</span>]*(-node.inputs[<span class="number">1</span>]**(-<span class="number">2</span>))</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="divscalar-1"><code>DivScalar</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad / <span class="variable language_">self</span>.scalar</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="matmul-1"><code>MatMul</code></h2>
<p>如果只有二维矩阵，其实不需要加if，但是存在高维矩阵的情况，打个比方，形状为<code>[3,2,5]</code>和<code>[5,1]</code>的矩阵相乘，得到的结果应该是<code>[3,2,1]</code>。但是在反向传播时，left_adjoint是正常的，而right_adjoint中<code>right_adjoint = transpose(node.inputs[0])@out_grad</code>，也就是<code>[3,5,2]@[3,2,1]</code>，得到的结果是<code>[3,5,1]</code>，但是期待得到的结果是<code>[5,1]</code>。如果按照之前说的凑维度，很容易想到是对第一个维度求和。如果再高维，如形状为<code>[4,3,2,5]</code>和<code>[5,1]</code>的矩阵相乘，那么right_adjoint得到的结果就是<code>[4,3,5,2]@[3,2,1]</code>即<code>[4,3,5,1]</code>，就是对前两个维度进行求和。因此写出以下代码，验证后发现正确。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    left_adjoint = out_grad@transpose(node.inputs[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(left_adjoint.shape) &gt; <span class="built_in">len</span>(node.inputs[<span class="number">0</span>].shape):</span><br><span class="line">        axes_to_sum = <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(left_adjoint.shape) - <span class="built_in">len</span>(node.inputs[<span class="number">0</span>].shape)))</span><br><span class="line">        left_adjoint = summation(left_adjoint,axes=axes_to_sum)</span><br><span class="line">    right_adjoint = transpose(node.inputs[<span class="number">0</span>])@out_grad</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(right_adjoint.shape) &gt; <span class="built_in">len</span>(node.inputs[<span class="number">1</span>].shape):</span><br><span class="line">        axes_to_sum = <span class="built_in">tuple</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(right_adjoint.shape) - <span class="built_in">len</span>(node.inputs[<span class="number">1</span>].shape)))</span><br><span class="line">        right_adjoint = summation(right_adjoint,axes=axes_to_sum)</span><br><span class="line">    <span class="keyword">return</span> left_adjoint, right_adjoint</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="summation-1"><code>Summation</code></h2>
<p>这个和之后的广播之类的操作一样，刚开始没有一点思路，不知道为什么求和操作还能有梯度。还是阅读<code>gradient_check</code>函数后得知怎么做。其实本质来说，这个运算的梯度也即输入的每一个元素对输出有多大影响。用常数举一个例子，例如<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.551ex" height="1.971ex" role="img" focusable="false" viewbox="0 -666 2895.6 871"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(1349.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2405.6,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>，那么每一个输入x对于y就是2倍的影响。同理，如果是<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="6.283ex" height="1.992ex" role="img" focusable="false" viewbox="0 -675.5 2777 880.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="mi" transform="translate(499,363) scale(0.707)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g></g><g data-mml-node="mo" transform="translate(1231.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2287,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></svg></mjx-container></span>，对于每一个x，可以用<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex;" xmlns="http://www.w3.org/2000/svg" width="7.97ex" height="2.939ex" role="img" focusable="false" viewbox="0 -946.2 3522.5 1298.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,398) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(572,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(1350,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g><g data-mml-node="mo" transform="translate(1790.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="msup" transform="translate(2568.7,0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"/></g><g data-mml-node="TeXAtom" transform="translate(499,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(572,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(1350,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></g><g data-mml-node="mrow" transform="translate(1440.9,-345) scale(0.707)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g><rect width="3282.5" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span>来获得当前这个输入对于输出有多大影响。当然，如果<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container></span>取极小值是可以通过这个式子获得准确的梯度计算公式。但是在写这个代码的时候我是直接带了具体数值进去，看一下不同输入是如何影响输出的，我举一个例子：</p>
<p>假如summation操作中，为了简化取<code>axis=none</code>，如果输入的矩阵为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.149ex;" xmlns="http://www.w3.org/2000/svg" width="6.914ex" height="5.43ex" role="img" focusable="false" viewbox="0 -1450 3056 2400"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mrow"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"/></g><g data-mml-node="mtable" transform="translate(528,0)"><g data-mml-node="mtr" transform="translate(0,700)"><g data-mml-node="mtd"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><g data-mml-node="mtr" transform="translate(0,-700)"><g data-mml-node="mtd"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mn"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"/></g></g></g></g><g data-mml-node="mo" transform="translate(2528,0) translate(0 -0.5)"><path data-c="5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"/></g></g></g></g></svg></mjx-container></span>，前向计算得到的结果是10。那么如果第一个元素1增加一个微小的<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container></span>，也就是得到的结果是<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="5.947ex" height="1.692ex" role="img" focusable="false" viewbox="0 -666 2628.4 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/></g><g data-mml-node="mo" transform="translate(1222.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(2222.4,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container></span>，不用赘述，梯度的数学表达式可以表示为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex;" xmlns="http://www.w3.org/2000/svg" width="11.717ex" height="3.167ex" role="img" focusable="false" viewbox="0 -1047.1 5179 1399.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mrow" transform="translate(220,516.8) scale(0.707)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mn" transform="translate(389,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/></g><g data-mml-node="mo" transform="translate(1389,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(2167,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g><g data-mml-node="mo" transform="translate(2573,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(2962,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mo" transform="translate(3740,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mn" transform="translate(4129,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/></g><g data-mml-node="mo" transform="translate(5129,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(5907,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g><g data-mml-node="mo" transform="translate(6313,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(2269.2,-345) scale(0.707)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g><rect width="4939" height="60" x="120" y="220"/></g></g></g></svg></mjx-container></span>即等于1，多测试几个数发现每一个元素对于最终的输出的贡献都为1，也就是说梯度为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -2.149ex;" xmlns="http://www.w3.org/2000/svg" width="6.914ex" height="5.43ex" role="img" focusable="false" viewbox="0 -1450 3056 2400"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mrow"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="5B" d="M247 -949V1450H516V1388H309V-887H516V-949H247Z"/></g><g data-mml-node="mtable" transform="translate(528,0)"><g data-mml-node="mtr" transform="translate(0,700)"><g data-mml-node="mtd"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g><g data-mml-node="mtr" transform="translate(0,-700)"><g data-mml-node="mtd"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mtd" transform="translate(1500,0)"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g><g data-mml-node="mo" transform="translate(2528,0) translate(0 -0.5)"><path data-c="5D" d="M11 1388V1450H280V-949H11V-887H218V1388H11Z"/></g></g></g></g></svg></mjx-container></span>，由于out_grad此时是一个常数，直接乘上去就行，相当于广播。</p>
<p>对于<code>axis != None</code>的情况，我们需要理解前向传播中求和操作如何改变张量维度：当沿着指定axis进行求和时，输出张量会消除这些维度（例如输入形状为[2,3]且axis=1时，输出会坍缩为[2]）。这导致反向传播时，上层传递来的out_grad会缺失这些被消除的维度。</p>
<p><strong>例如输入为[[1,2],[3,4]]沿axis=1求和得到[3,7]，此时out_grad的shape是(2,)。但原始输入的每个元素（如位置[0,1]的2）对输出的贡献是沿axis=1方向的——当梯度从结果[3,7]传回时，每个元素的梯度需要分配到对应行的所有位置。</strong></p>
<p><strong>具体来说，反向传播的数学本质是：若输入元素x_i参与了n个输出元素的计算，则x_i的梯度是这些n个输出梯度之和。但在求和操作的特殊情况下，每个输入元素恰好只贡献给一个输出元素（即对应坍缩后的位置），因此梯度直接复制到所有参与求和的维度。</strong></p>
<blockquote>
<h3 id="假设前向传播的求和操作如下">假设前向传播的求和操作如下：</h3>
<p><strong>输入矩阵</strong>：<code>x = [[1, 2], [3, 4]]</code> <strong>操作</strong>：沿 <code>axis=1</code> 求和（即对每行求和） <strong>输出</strong>：<code>sum = [3, 7]</code> <strong>反向传播时</strong>：假设输出的梯度（<code>out_grad</code>）为 <code>[0.5, 1.5]</code>，求输入 <code>x</code> 的梯度。</p>
<hr>
<h3 id="反向传播的关键分析">反向传播的关键分析</h3>
<h4 id="普通操作的情况对比">普通操作的情况（对比）</h4>
<p>假设某个操作中，<strong>一个输入元素会影响多个输出元素</strong>。例如矩阵乘法：</p>
<ul>
<li>输入 <code>x = [x1, x2]</code></li>
<li>权重矩阵 <code>W = [[w11, w12], [w21, w22]]</code></li>
<li>输出 <code>y = [w11*x1 + w21*x2, w12*x1 + w22*x2]</code></li>
</ul>
<p>此时 <code>x1</code> 同时影响 <code>y1</code> 和 <code>y2</code>，反向传播时 <code>x1</code> 的梯度是 <code>∂y1/∂x1 * out_grad1 + ∂y2/∂x1 * out_grad2</code>（即多个输出梯度的<strong>累加</strong>）。</p>
<hr>
<h4 id="求和操作的特殊性">求和操作的特殊性</h4>
<p>在求和操作中，<strong>每个输入元素只影响一个输出元素</strong>。例如：</p>
<ul>
<li>输入元素 <code>x[0,0] = 1</code> 只参与输出 <code>sum[0] = 3</code> 的计算</li>
<li>输入元素 <code>x[0,1] = 2</code> 也只参与输出 <code>sum[0] = 3</code> 的计算</li>
<li>输入元素 <code>x[1,0] = 3</code> 和 <code>x[1,1] = 4</code> 只参与输出 <code>sum[1] = 7</code> 的计算</li>
</ul>
<p>此时反向传播的规则是： <strong>输入元素的梯度 = 对应的输出元素的梯度</strong></p>
<ul>
<li><code>x[0,0]</code> 的梯度 = <code>out_grad[0]</code></li>
<li><code>x[0,1]</code> 的梯度 = <code>out_grad[0]</code></li>
<li><code>x[1,0]</code> 的梯度 = <code>out_grad[1]</code></li>
<li><code>x[1,1]</code> 的梯度 = <code>out_grad[1]</code></li>
</ul>
<p>最终梯度矩阵： <code>x_grad = [[0.5, 0.5], [1.5, 1.5]]</code></p>
</blockquote>
<p>为了实现这一点，梯度计算需要两步操作：</p>
<ol type="1">
<li><strong>维度重建</strong>：通过reshape在out_grad被消除的轴上插入尺寸1（例如将(2,)转为(2,1)），这恢复了被求和操作消除的维度信息；</li>
<li><strong>广播填充</strong>：通过乘以全1矩阵，将out_grad沿原始求和的轴进行广播（例如(2,1)广播到(2,2)），使得每个原始输入位置都能获得对应的梯度分量。</li>
</ol>
<p><strong>这种维度操作的本质是：将输出梯度在坍缩的轴上进行”逆向坍缩”，把标量梯度值平摊到该轴所有位置，最终通过广播恢复与原输入一致的维度结构。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">        <span class="comment"># 获取输入张量的形状</span></span><br><span class="line">        a_shape = node.inputs[<span class="number">0</span>].shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果没有指定 axis（axes=None），说明前向求和是对所有元素求和，</span></span><br><span class="line">        <span class="comment"># 此时 out_grad 为标量，我们需要将其广播到整个输入张量的形状。</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.axes <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 利用全1的张量乘以 out_grad 来实现广播</span></span><br><span class="line">            <span class="keyword">return</span> out_grad * array_api.ones(a_shape)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果指定了 axis，那么 axes 可能是整数或者元组，</span></span><br><span class="line">        <span class="comment"># 为了统一处理，将其转换为元组形式。</span></span><br><span class="line">        axes = <span class="variable language_">self</span>.axes <span class="keyword">if</span> <span class="built_in">isinstance</span>(<span class="variable language_">self</span>.axes, <span class="built_in">tuple</span>) <span class="keyword">else</span> (<span class="variable language_">self</span>.axes,)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造 out_grad 应该重新排列的形状：</span></span><br><span class="line">        <span class="comment"># 对于输入张量中的每个维度，如果该维度在 axes 中，</span></span><br><span class="line">        <span class="comment"># 则在输出梯度中该位置需要插入一个单例维度（即尺寸为 1），</span></span><br><span class="line">        <span class="comment"># 否则取出 out_grad 原有的对应尺寸。</span></span><br><span class="line">        new_shape = []</span><br><span class="line">        out_grad_shape_iter = <span class="built_in">iter</span>(out_grad.shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(a_shape)):</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> axes:</span><br><span class="line">                new_shape.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                new_shape.append(<span class="built_in">next</span>(out_grad_shape_iter))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将 out_grad reshape 到 new_shape 后利用广播乘以全1张量扩展到原始形状</span></span><br><span class="line">        <span class="keyword">return</span> out_grad.reshape(new_shape) * array_api.ones(a_shape)</span><br></pre></td></tr></table></figure>
<h2 id="broadcastto-1"><code>BroadcastTo</code></h2>
<p>做的时候感觉也挺费脑的，但是可以参考上面的求和操作，不再赘述。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">        original_shape = node.inputs[<span class="number">0</span>].shape</span><br><span class="line">        broadcasted_shape = <span class="variable language_">self</span>.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找到所有需要求和的轴（被扩展的轴）</span></span><br><span class="line">        axes = []</span><br><span class="line">        original_ndim = <span class="built_in">len</span>(original_shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(broadcasted_shape)):</span><br><span class="line">            <span class="keyword">if</span> i &gt;= original_ndim <span class="keyword">or</span> original_shape[i] != broadcasted_shape[i]:</span><br><span class="line">                axes.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 沿这些轴求和，恢复原始形状</span></span><br><span class="line">        grad = summation(out_grad, axes=<span class="built_in">tuple</span>(axes))</span><br><span class="line">        <span class="keyword">return</span> grad.reshape(original_shape)</span><br></pre></td></tr></table></figure>
<h2 id="reshape-1"><code>Reshape</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> reshape(out_grad, node.inputs[<span class="number">0</span>].shape)</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="negate-1"><code>Negate</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span> * out_grad</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="transpose-1"><code>Transpose</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> transpose(out_grad, <span class="variable language_">self</span>.axes)</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="log-1"><code>Log</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad * power_scalar(node.inputs[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="exp-1"><code>Exp</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad * exp(node.inputs[<span class="number">0</span>])</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h2 id="ewisepow-1"><code>EWisePow</code></h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">self, out_grad, node</span>):</span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="keyword">return</span> out_grad * (node.inputs[<span class="number">1</span>]) * power(node.inputs[<span class="number">0</span>], node.inputs[<span class="number">1</span>]-<span class="number">1</span>), \</span><br><span class="line">           out_grad * log(node.inputs[<span class="number">0</span>]) * power(node.inputs[<span class="number">0</span>], node.inputs[<span class="number">1</span>])</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h1 id="拓扑排序">拓扑排序</h1>
<p>这个没啥说的，就是个递归，挺简单的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_topo_sort</span>(<span class="params">node_list: <span class="type">List</span>[Value]</span>) -&gt; <span class="type">List</span>[Value]:</span><br><span class="line">    <span class="string">"""Given a list of nodes, return a topological sort list of nodes ending in them.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    A simple algorithm is to do a post-order DFS traversal on the given nodes,</span></span><br><span class="line"><span class="string">    going backwards based on input edges. Since a node is added to the ordering</span></span><br><span class="line"><span class="string">    after all its predecessors are traversed due to post-order DFS, we get a topological</span></span><br><span class="line"><span class="string">    sort.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">    <span class="comment"># 输入是汇点</span></span><br><span class="line"></span><br><span class="line">    topo_order = []</span><br><span class="line">    topo_sort_dfs(node_list[<span class="number">0</span>],<span class="literal">False</span>,topo_order)</span><br><span class="line">    <span class="keyword">return</span> topo_order</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">topo_sort_dfs</span>(<span class="params">node, visited, topo_order</span>):</span><br><span class="line">    <span class="string">"""Post-order DFS"""</span></span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">    <span class="keyword">if</span> visited <span class="keyword">or</span> node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 可以不要，但是要在下面的input[0]加上判断，但是用这句话可以省事</span></span><br><span class="line">    <span class="keyword">if</span> node.is_leaf():</span><br><span class="line">        topo_order.append(node)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    topo_sort_dfs(node.inputs[<span class="number">0</span>],node.inputs[<span class="number">0</span>] <span class="keyword">in</span> topo_order,topo_order)</span><br><span class="line">    <span class="comment"># 很重要！</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(node.inputs)&gt;<span class="number">1</span>:</span><br><span class="line">        topo_sort_dfs(node.inputs[<span class="number">1</span>],node.inputs[<span class="number">1</span>] <span class="keyword">in</span> topo_order,topo_order)</span><br><span class="line">    topo_order.append(node)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="反向模式微分实现">反向模式微分实现</h1>
<p>这个完全按照图上的伪代码写就行了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_gradient_of_variables</span>(<span class="params">output_tensor, out_grad</span>):</span><br><span class="line">    <span class="string">"""Take gradient of output node with respect to each node in node_list.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Store the computed result in the grad field of each Variable.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># a map from node to a list of gradient contributions from each output node</span></span><br><span class="line">    node_to_output_grads_list: <span class="type">Dict</span>[Tensor, <span class="type">List</span>[Tensor]] = {}</span><br><span class="line">    <span class="comment"># Special note on initializing gradient of</span></span><br><span class="line">    <span class="comment"># We are really taking a derivative of the scalar reduce_sum(output_node)</span></span><br><span class="line">    <span class="comment"># instead of the vector output_node. But this is the common case for loss function.</span></span><br><span class="line">    node_to_output_grads_list[output_tensor] = [out_grad]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Traverse graph in reverse topological order given the output_node that we are taking gradient wrt.</span></span><br><span class="line">    reverse_topo_order = <span class="built_in">list</span>(<span class="built_in">reversed</span>(find_topo_sort([output_tensor])))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> reverse_topo_order:</span><br><span class="line">        node.grad = sum_node_list(node_to_output_grads_list[node])</span><br><span class="line">        <span class="keyword">if</span> node.is_leaf():</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        grads = node.op.gradient(node.grad,node)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(grads,<span class="type">Tuple</span>):</span><br><span class="line">            <span class="keyword">for</span> grad, index <span class="keyword">in</span> <span class="built_in">zip</span>(grads, <span class="built_in">range</span>(<span class="built_in">len</span>(grads))):</span><br><span class="line">                <span class="keyword">if</span> node.inputs[index] <span class="keyword">in</span> node_to_output_grads_list:</span><br><span class="line">                    node_to_output_grads_list[node.inputs[index]].append(grad)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    node_to_output_grads_list[node.inputs[index]] = [grad]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> node.inputs[<span class="number">0</span>] <span class="keyword">in</span> node_to_output_grads_list:</span><br><span class="line">                node_to_output_grads_list[node.inputs[<span class="number">0</span>]].append(grads)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                node_to_output_grads_list[node.inputs[<span class="number">0</span>]] = [grads]</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h1 id="softmax-loss">Softmax Loss</h1>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax_loss</span>(<span class="params">Z, y_one_hot</span>):</span><br><span class="line">    <span class="string">"""Return softmax loss.  Note that for the purposes of this assignment,</span></span><br><span class="line"><span class="string">    you don't need to worry about "nicely" scaling the numerical properties</span></span><br><span class="line"><span class="string">    of the log-sum-exp computation, but can just compute this directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        Z (ndl.Tensor[np.float32]): 2D Tensor of shape</span></span><br><span class="line"><span class="string">            (batch_size, num_classes), containing the logit predictions for</span></span><br><span class="line"><span class="string">            each class.</span></span><br><span class="line"><span class="string">        y (ndl.Tensor[np.int8]): 2D Tensor of shape (batch_size, num_classes)</span></span><br><span class="line"><span class="string">            containing a 1 at the index of the true label of each example and</span></span><br><span class="line"><span class="string">            zeros elsewhere.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Average softmax loss over the sample. (ndl.Tensor[np.float32])</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">    Z_exp = ndl.exp(Z)</span><br><span class="line">    Z_exp_sum = ndl.summation(Z_exp, axes=<span class="number">1</span>)</span><br><span class="line">    Z_exp_sum_log = ndl.log(Z_exp_sum)</span><br><span class="line">    Z_y = ndl.summation(Z*y_one_hot,axes = <span class="number">1</span>)</span><br><span class="line">    loss = ndl.summation(Z_exp_sum_log - Z_y)/Z.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>
<h1 id="用needle来重写sgd">用needle来重写SGD</h1>
<p>这个函数我改了很久，主要是关于numpy和ndl.tensor之间经常发生一些混用，用的时候一定小心。另外不同维度的运算在needle里面要进行现实的广播，这一点也很容易出错。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nn_epoch</span>(<span class="params">X, y, W1, W2, lr=<span class="number">0.1</span>, batch=<span class="number">100</span></span>):</span><br><span class="line">    <span class="string">"""Run a single epoch of SGD for a two-layer neural network defined by the</span></span><br><span class="line"><span class="string">    weights W1 and W2 (with no bias terms):</span></span><br><span class="line"><span class="string">        logits = ReLU(X * W1) * W1</span></span><br><span class="line"><span class="string">    The function should use the step size lr, and the specified batch size (and</span></span><br><span class="line"><span class="string">    again, without randomizing the order of X).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        X (np.ndarray[np.float32]): 2D input array of size</span></span><br><span class="line"><span class="string">            (num_examples x input_dim).</span></span><br><span class="line"><span class="string">        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)</span></span><br><span class="line"><span class="string">        W1 (ndl.Tensor[np.float32]): 2D array of first layer weights, of shape</span></span><br><span class="line"><span class="string">            (input_dim, hidden_dim)</span></span><br><span class="line"><span class="string">        W2 (ndl.Tensor[np.float32]): 2D array of second layer weights, of shape</span></span><br><span class="line"><span class="string">            (hidden_dim, num_classes)</span></span><br><span class="line"><span class="string">        lr (float): step size (learning rate) for SGD</span></span><br><span class="line"><span class="string">        batch (int): size of SGD mini-batch</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tuple: (W1, W2)</span></span><br><span class="line"><span class="string">            W1: ndl.Tensor[np.float32]</span></span><br><span class="line"><span class="string">            W2: ndl.Tensor[np.float32]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### BEGIN YOUR SOLUTION</span></span><br><span class="line">    <span class="comment"># raise NotImplementedError()</span></span><br><span class="line">    num_classes = W2.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, X.shape[<span class="number">0</span>], batch):</span><br><span class="line">        end = <span class="built_in">min</span>(i + batch, X.shape[<span class="number">0</span>])</span><br><span class="line">        X_batch = X[i:end, :]</span><br><span class="line">        y_batch = y[i:end]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建one-hot标签</span></span><br><span class="line">        I_y = np.zeros((<span class="built_in">len</span>(y_batch), num_classes), dtype=np.float32)</span><br><span class="line">        I_y[np.arange(<span class="built_in">len</span>(y_batch)), y_batch] = <span class="number">1.0</span></span><br><span class="line">        I_y = ndl.Tensor(I_y)  <span class="comment"># 转换为Tensor</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向计算</span></span><br><span class="line">        X_tensor = ndl.Tensor(X_batch)</span><br><span class="line">        Z_1 = ndl.relu(ndl.matmul(X_tensor, W1))</span><br><span class="line">        logits = ndl.matmul(Z_1, W2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算softmax梯度</span></span><br><span class="line">        exp_logits = ndl.exp(logits)</span><br><span class="line">        sum_exp = ndl.summation(exp_logits, axes=<span class="number">1</span>)</span><br><span class="line">        softmax = exp_logits / ndl.broadcast_to(ndl.reshape(sum_exp,(exp_logits.shape[<span class="number">0</span>],<span class="number">1</span>)),exp_logits.shape)</span><br><span class="line">        G_2 = softmax - I_y</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算隐藏层梯度</span></span><br><span class="line">        relu_mask = (Z_1.numpy() &gt; <span class="number">0</span>).astype(np.float32)</span><br><span class="line">        G_1 = ndl.Tensor(relu_mask) * ndl.matmul(G_2, W2.transpose())</span><br><span class="line"></span><br><span class="line">        delta_W1 = ndl.matmul(ndl.transpose(X_tensor), G_1) / (end - i)</span><br><span class="line">        delta_W2 = ndl.matmul(ndl.transpose(Z_1), G_2) / (end - i)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新权重（确保结果为Tensor）</span></span><br><span class="line">        W1 = ndl.Tensor(W1.numpy() - (lr * delta_W1).numpy())</span><br><span class="line">        W2 = ndl.Tensor(W2.numpy() - (lr * delta_W2).numpy())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> W1, W2</span><br><span class="line">    <span class="comment">### END YOUR SOLUTION</span></span><br></pre></td></tr></table></figure>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://gladiouszhang.github.io/2025/04/04/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E4%BA%94%E7%AB%A0-hw1/" title="CMU-深度学习系统-第五章&amp;hw1" target="_blank" rel="external">http://gladiouszhang.github.io/2025/04/04/CMU-深度学习系统-第五章-hw1/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/gladiouszhang" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/gladiouszhang" target="_blank"><span class="text-dark">Shuyu Zhang (张舒俞)</span><small class="ml-1x">学生</small></a></h3>
        <div>西安电子科技大学硕士在读</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/2025/04/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-%E7%AC%AC%E5%85%AD%E7%AB%A0/" title="CMU-深度学习系统-第六章"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/2025/03/09/CMU-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F-hw0/" title="CMU-深度学习系统-hw0"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/gladiouszhang" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://weibo.com/n/Gladious_Zhang" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        &copy; 2025 张舒俞
        
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: true,
    notify: true,
    appId: 'GNXx14HnrCFqRSxo50bKuXAt-gzGzoHsz',
    appKey: '68hg4AgzSBf1I5q0GwkpKIyu',
    placeholder: '欢迎留下您的评论😊',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: false,
    serverURLs: ''
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>